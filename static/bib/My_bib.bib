@misc{pearlwhy,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}

@article{hernan2019second,
  title={A second chance to get causal inference right: a classification of data science tasks},
  author={Hern{\'a}n, Miguel A and Hsu, John and Healy, Brian},
  journal={Chance},
  volume={32},
  number={1},
  pages={42--49},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{shmueli2010explain,
  title={To explain or to predict?},
  author={Shmueli, Galit},
  journal={Statistical science},
  volume={25},
  number={3},
  pages={289--310},
  year={2010},
  publisher={Institute of Mathematical Statistics}
}

@misc{hernan2022causal,
  title={Causal Inference: What If. Published online 2020},
  author={Hern{\'a}n, MA and Robins, JM},
  year={2022}
}

@article{vanderLaanRubin,
url = {https://doi.org/10.2202/1557-4679.1043},
title = {Targeted Maximum Likelihood Learning},
title = {},
author = {Mark J. van der Laan and Daniel Rubin},
volume = {2},
number = {1},
journal = {The International Journal of Biostatistics},
doi = {doi:10.2202/1557-4679.1043},
year = {2006},
lastchecked = {2022-12-03}
}

@article{Chernozhukov,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@article{IvanDiaz,
    author = {Díaz, Iván},
    title = "{Machine learning in the estimation of causal effects: targeted minimum loss-based estimation and double/debiased machine learning}",
    journal = {Biostatistics},
    volume = {21},
    number = {2},
    pages = {353-358},
    year = {2019},
    month = {11},
    abstract = "{In recent decades, the fields of statistical and machine learning have seen a revolution in the development of data-adaptive regression methods that have optimal performance under flexible, sometimes minimal, assumptions on the true regression functions. These developments have impacted all areas of applied and theoretical statistics and have allowed data analysts to avoid the biases incurred under the pervasive practice of parametric model misspecification. In this commentary, I discuss issues around the use of data-adaptive regression in estimation of causal inference parameters. To ground ideas, I focus on two estimation approaches with roots in semi-parametric estimation theory: targeted minimum loss-based estimation (TMLE; van der Laan and Rubin, 2006) and double/debiased machine learning (DML; Chernozhukov and others, 2018). This commentary is not comprehensive, the literature on these topics is rich, and there are many subtleties and developments which I do not address. These two frameworks represent only a small fraction of an increasingly large number of methods for causal inference using machine learning. To my knowledge, they are the only methods grounded in statistical semi-parametric theory that also allow unrestricted use of data-adaptive regression techniques.}",
    issn = {1465-4644},
    doi = {10.1093/biostatistics/kxz042},
    url = {https://doi.org/10.1093/biostatistics/kxz042},
    eprint = {https://academic.oup.com/biostatistics/article-pdf/21/2/353/32914770/kxz042.pdf},
}

